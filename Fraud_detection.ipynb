{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P8g8wuX2y_l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/PS_20174392719_1491204439457_log.csv')\n",
        "data = data1.sample(frac=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "hFwi2JDw5mux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore the dataset\n",
        "data.head(n=10)"
      ],
      "metadata": {
        "id": "aQYU3P3s5wvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "1BOMvA-p5ydy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.hist (bins=50, figsize=(15,15), color = 'green')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RyAHAMT-51zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Distribution of transactions wrt to source account\n",
        "data['nameOrig'].value_counts().hist (bins=500, figsize=(15,5), color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Euywjc0o539Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['nameOrig'].value_counts().describe()"
      ],
      "metadata": {
        "id": "hr5irTQE56wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution of transactions wrt to dest account\n",
        "data['nameDest'].value_counts().hist (bins=500, figsize=(15,5), color = 'purple')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wxIhAofl588h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore the class distribution\n",
        "data.isFraud.value_counts().plot.pie(autopct='%.2f',figsize=(5, 5), colors=[\"purple\",\"cyan\"], explode=[0,.1])\n",
        "plt.title('Class Distribution')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "heTE8r8q5-6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if there is anu null values\n",
        "data.isna().sum().sum()\n",
        "\n",
        "\n",
        "\n",
        "#check for duplicate values\n",
        "data.duplicated(keep='first').any()"
      ],
      "metadata": {
        "id": "5nwiJ4JY6Bn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter data by the labels. Safe and Fraud transaction\n",
        "safe = data[data['isFraud']==0]\n",
        "fraud = data[data['isFraud']==1]\n",
        "#See the frequency of the transactions for each class on the same plot.\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.distplot(safe.step, label=\"Safe Transaction\")\n",
        "sns.distplot(fraud.step, label='Fraud Transaction')\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.title('Distribution of Transactions over the Time')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "LpFzAt1R6Dt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Type of Transactions for fraud\n",
        "#checking type of fraud transactions\n",
        "fraud.type.value_counts()"
      ],
      "metadata": {
        "id": "p0UwvzZW6Grs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering only transfer and cash_out data\n",
        "data_by_type=data[data['type'].isin(['TRANSFER','CASH_OUT'])]"
      ],
      "metadata": {
        "id": "BHdQ9TJy6Iqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(['isFraud','nameOrig', 'nameDest'], axis=1)\n",
        "y = data['isFraud']"
      ],
      "metadata": {
        "id": "jsSA8on26Kj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.get_dummies(X, columns=['type'], drop_first=True)"
      ],
      "metadata": {
        "id": "dZkZxJoN6NB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "ZNYERyWJ6PB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n"
      ],
      "metadata": {
        "id": "TPiBiTzB6Qef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "Q49BPx6q6SKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check class distribution before RUS\n",
        "print(\"Class Distribution before RUS:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "# Check class distribution after RUS\n",
        "print(\"\\nClass Distribution after RUS:\")\n",
        "print(pd.Series(y_resampled).value_counts())\n",
        "\n",
        "# Visualize class distribution before and after RUS\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Before RUS\n",
        "axes[0].bar(y_train.value_counts().index, y_train.value_counts().values, color=['blue', 'green'])\n",
        "axes[0].set_title('Class Distribution before RUS')\n",
        "axes[0].set_xlabel('Class')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "# After RUS\n",
        "axes[1].bar(pd.Series(y_resampled).value_counts().index, pd.Series(y_resampled).value_counts().values, color=['blue', 'green'])\n",
        "axes[1].set_title('Class Distribution after RUS')\n",
        "axes[1].set_xlabel('Class')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kU9CjxQk6VGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "96zu8JTI6XoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_resampled, y_resampled)"
      ],
      "metadata": {
        "id": "8wItYNLz6Zab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "zrEz1TKO6a_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "# Confusion Matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Non-Fraud', 'Fraud'],\n",
        "            yticklabels=['Non-Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QSfC4QDs6dks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# KNN Model without Feature Scaling\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "# Confusion Matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Non-Fraud', 'Fraud'],\n",
        "            yticklabels=['Non-Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gE3Ts3Ui6gzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Model on RUS data\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Non-Fraud', 'Fraud'],\n",
        "            yticklabels=['Non-Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NxreZh4P6jGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "LF7u6jds6lhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the resampled data\n",
        "rf_model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# Predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Non-Fraud', 'Fraud'],\n",
        "            yticklabels=['Non-Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4aMQUzxM6nRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X contains your dataset\n",
        "\n",
        "# Step 1: Use k-means to partition the data into clusters\n",
        "n_clusters = 5  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=n_clusters)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Step 2: Identify clusters that are likely to contain outliers\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Calculate the distances of instances from cluster centers\n",
        "neigh = NearestNeighbors(n_neighbors=1)\n",
        "neigh.fit(cluster_centers)\n",
        "distances, _ = neigh.kneighbors(X)\n",
        "\n",
        "# Determine a threshold for outlier detection (e.g., distance above a certain percentile)\n",
        "threshold = np.percentile(distances, 95)\n",
        "\n",
        "# Step 3: Detect outliers based on distance threshold\n",
        "outlier_indices = np.where(distances > threshold)[0]\n",
        "\n",
        "# Print the indices of outliers\n",
        "print(\"Indices of outliers:\", outlier_indices)\n",
        "\n",
        "# Convert outlier indices to a binary array indicating outliers (1) and inliers (0)\n",
        "y_pred_binary = np.zeros(len(X))\n",
        "y_pred_binary[outlier_indices] = 1\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y, y_pred_binary)\n",
        "recall = recall_score(y, y_pred_binary)\n",
        "f1 = f1_score(y, y_pred_binary)\n",
        "accuracy = accuracy_score(y, y_pred_binary)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "GkzogdW76qUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyod\n",
        "from pyod.models.cblof import CBLOF\n",
        "\n",
        "\n",
        "# Assuming X contains your dataset and y contains true labels for outliers\n",
        "\n",
        "# Instantiate the CBLOF model\n",
        "cblof_model = CBLOF(contamination=0.1)  # Adjust contamination parameter as needed\n",
        "\n",
        "# Fit the model\n",
        "cblof_model.fit(X)\n",
        "\n",
        "# Predict outliers\n",
        "y_pred_binary = cblof_model.predict(X)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y, y_pred_binary)\n",
        "recall = recall_score(y, y_pred_binary)\n",
        "f1 = f1_score(y, y_pred_binary)\n",
        "accuracy = accuracy_score(y, y_pred_binary)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "u-BObVAC6tzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyod.models.abod import ABOD\n",
        "\n",
        "\n",
        "# Assuming X contains your dataset and y contains true labels for outliers\n",
        "\n",
        "# Instantiate the ABOD model\n",
        "abod_model = ABOD(contamination=0.1)  # Adjust contamination parameter as needed\n",
        "\n",
        "# Fit the model\n",
        "abod_model.fit(X)\n",
        "\n",
        "# Predict outliers\n",
        "y_pred_binary = abod_model.predict(X)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y, y_pred_binary)\n",
        "recall = recall_score(y, y_pred_binary)\n",
        "f1 = f1_score(y, y_pred_binary)\n",
        "accuracy = accuracy_score(y, y_pred_binary)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "gESUiWf-6wua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyod.models.hbos import HBOS\n",
        "\n",
        "\n",
        "# Assuming X contains your dataset and y contains true labels for outliers\n",
        "\n",
        "# Instantiate the HBOS model\n",
        "hbos_model = HBOS(contamination=0.1)  # Adjust contamination parameter as needed\n",
        "\n",
        "# Fit the model\n",
        "hbos_model.fit(X)\n",
        "\n",
        "# Predict outliers\n",
        "y_pred_binary = hbos_model.predict(X)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y, y_pred_binary)\n",
        "recall = recall_score(y, y_pred_binary)\n",
        "f1 = f1_score(y, y_pred_binary)\n",
        "accuracy = accuracy_score(y, y_pred_binary)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "PdtkWu6n6yvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "\n",
        "# Assuming X contains your dataset and y contains true labels for outliers\n",
        "\n",
        "# Instantiate the Isolation Forest model\n",
        "isolation_forest_model = IsolationForest(contamination=0.1)  # Adjust contamination parameter as needed\n",
        "\n",
        "# Fit the model\n",
        "isolation_forest_model.fit(X)\n",
        "\n",
        "# Predict outliers\n",
        "y_pred_binary = isolation_forest_model.predict(X)\n",
        "y_pred_binary[y_pred_binary == 1] = 0  # Inliers\n",
        "y_pred_binary[y_pred_binary == -1] = 1  # Outliers\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y, y_pred_binary)\n",
        "recall = recall_score(y, y_pred_binary)\n",
        "f1 = f1_score(y, y_pred_binary)\n",
        "accuracy = accuracy_score(y, y_pred_binary)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y, y_pred_binary)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal', 'Outlier'],\n",
        "            yticklabels=['Normal', 'Outlier'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xTrmVtSQ61K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "id": "j-tMG-kO7kKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "plUP38dn7mP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# Assuming X contains your dataset and y contains true labels for outliers\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "input_dim = X.shape[1]  # Number of features\n",
        "encoding_dim = 32  # Number of neurons in the hidden layer\n",
        "\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
        "\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Fit the model\n",
        "autoencoder.fit(X, X, epochs=10, batch_size=32, shuffle=True, validation_split=0.1)\n",
        "\n",
        "# Use the autoencoder to reconstruct the data\n",
        "reconstructed_X = autoencoder.predict(X)\n",
        "\n",
        "# Calculate reconstruction errors\n",
        "mse = np.mean(np.square(X - reconstructed_X), axis=1)\n",
        "\n",
        "# Define a threshold for outlier detection\n",
        "threshold = np.percentile(mse, 95)\n",
        "\n",
        "# Predict outliers based on the reconstruction errors\n",
        "y_pred_binary = (mse > threshold).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y, y_pred_binary)\n",
        "recall = recall_score(y, y_pred_binary)\n",
        "f1 = f1_score(y, y_pred_binary)\n",
        "accuracy = accuracy_score(y, y_pred_binary)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y, y_pred_binary)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal', 'Outlier'],\n",
        "            yticklabels=['Normal', 'Outlier'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wf-HfMGD7oCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the Generator and Discriminator Networks\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(X.shape[1])\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X.shape[1],)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the GAN Model\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "def make_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = tf.keras.Input(shape=(100,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = tf.keras.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return gan\n",
        "\n",
        "gan = make_gan(generator, discriminator)\n",
        "\n",
        "# Compile the Generator and Discriminator models\n",
        "generator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the GAN\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    generated_data = generator.predict(noise)\n",
        "\n",
        "    real_data = X_scaled[np.random.randint(0, X_scaled.shape[0], batch_size)]\n",
        "\n",
        "    combined_data = np.concatenate([real_data, generated_data])\n",
        "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
        "\n",
        "    discriminator_loss = discriminator.train_on_batch(combined_data, labels)\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "    misleading_targets = np.zeros((batch_size, 1))\n",
        "\n",
        "    gan_loss = gan.train_on_batch(noise, misleading_targets)\n",
        "\n",
        "# Generate Synthetic Data\n",
        "synthetic_data = generator.predict(np.random.normal(0, 1, (1000, 100)))\n",
        "\n",
        "# Identify Outliers (Example: comparing real data with synthetic data)\n",
        "# You can use a threshold or other anomaly detection techniques to identify outliers\n",
        "\n",
        "# Example evaluation metrics\n",
        "y_true = np.zeros((X_scaled.shape[0], 1))  # Assuming all data is normal\n",
        "y_pred = np.zeros((X_scaled.shape[0], 1))  # Assuming all data is normal\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "sPR89sk97qbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Generator and Discriminator Networks with dropout\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(X.shape[1])\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X.shape[1],)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the GAN Model\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "def make_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = tf.keras.Input(shape=(100,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = tf.keras.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return gan\n",
        "\n",
        "gan = make_gan(generator, discriminator)\n",
        "\n",
        "# Compile the Generator and Discriminator models\n",
        "generator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the GAN with early stopping\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "history = gan.fit(np.random.normal(0, 1, (X_train.shape[0], 100)), np.ones((X_train.shape[0], 1)),\n",
        "                  validation_data=(np.random.normal(0, 1, (X_test.shape[0], 100)), np.ones((X_test.shape[0], 1))),\n",
        "                  epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "# Generate Synthetic Data\n",
        "synthetic_data = generator.predict(np.random.normal(0, 1, (X_scaled.shape[0], 100)))\n",
        "\n",
        "# Identify Outliers (Example: comparing real data with synthetic data)\n",
        "# You can use a threshold or other anomaly detection techniques to identify outliers\n",
        "\n",
        "# Example evaluation metrics\n",
        "y_true = np.zeros((X_scaled.shape[0], 1))  # Assuming all data is normal\n",
        "y_pred = np.zeros((X_scaled.shape[0], 1))  # Assuming all data is normal\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "uIVe2yYq7ssG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply data augmentation\n",
        "X_augmented = X_scaled + np.random.normal(0, 0.1, size=X_scaled.shape)\n",
        "\n",
        "# Combine original and augmented data\n",
        "X_combined = np.concatenate([X_scaled, X_augmented])\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test = train_test_split(X_combined, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Generator and Discriminator Networks with dropout\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(X.shape[1])\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X.shape[1],)),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the GAN Model with Wasserstein loss\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "def make_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = tf.keras.Input(shape=(100,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = tf.keras.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.00005), loss='binary_crossentropy')\n",
        "    return gan\n",
        "\n",
        "gan = make_gan(generator, discriminator)\n",
        "\n",
        "# Compile the Generator and Discriminator models\n",
        "generator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the GAN with early stopping and augmented data\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "history = gan.fit(np.random.normal(0, 1, (X_train.shape[0], 100)), np.ones((X_train.shape[0], 1)),\n",
        "                  validation_data=(np.random.normal(0, 1, (X_test.shape[0], 100)), np.ones((X_test.shape[0], 1))),\n",
        "                  epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "# Generate Synthetic Data\n",
        "synthetic_data = generator.predict(np.random.normal(0, 1, (X_scaled.shape[0], 100)))\n",
        "\n",
        "# Identify Outliers (Example: comparing real data with synthetic data)\n",
        "# You can use a threshold or other anomaly detection techniques to identify outliers\n",
        "\n",
        "# Example evaluation metrics\n",
        "y_true = np.zeros((X_scaled.shape[0], 1))  # Assuming all data is normal\n",
        "y_pred = np.zeros((X_scaled.shape[0], 1))  # Assuming all data is normal\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "fcQ9M6VO7uun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define Generator and Discriminator architectures\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(X.shape[1], activation='sigmoid')  # Output layer\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(256, activation='relu', input_shape=(X.shape[1],)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define MO-GAAL model\n",
        "def make_mogaal_model(generator, discriminator):\n",
        "    gan_input = tf.keras.Input(shape=(100,))\n",
        "    x = generator(gan_input)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = tf.keras.Model(gan_input, gan_output)\n",
        "    gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return gan\n",
        "\n",
        "# Define active learning strategy\n",
        "class ActiveLearningStrategy:\n",
        "    def __init__(self, X_unlabeled):\n",
        "        self.X_unlabeled = X_unlabeled\n",
        "\n",
        "    def select_samples(self, num_samples):\n",
        "        # Implement your active learning strategy here\n",
        "        # For example, select samples with highest uncertainty\n",
        "        selected_indices = np.random.choice(len(self.X_unlabeled), num_samples, replace=False)\n",
        "        return self.X_unlabeled[selected_indices]\n",
        "\n",
        "# Load data (labeled and unlabeled)\n",
        "paysim_data = pd.read_csv(\"data1.csv\")\n",
        "X_labeled = paysim_data.drop(['isFraud', 'nameOrig', 'nameDest', 'type'], axis=1).values\n",
        "y_labeled = paysim_data['isFraud'].values\n",
        "X_unlabeled = paysim_data.drop(['isFraud', 'nameOrig', 'nameDest', 'type'], axis=1).values\n",
        "\n",
        "# Train/test split for labeled data (for evaluation)\n",
        "X_train_labeled, X_test_labeled, y_train_labeled, y_test_labeled = train_test_split(X_labeled, y_labeled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define models\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "mogaal_model = make_mogaal_model(generator, discriminator)\n",
        "\n",
        "# Active learning strategy\n",
        "active_learning_strategy = ActiveLearningStrategy(X_unlabeled)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "num_active_samples = 100  # Number of samples to label at each iteration\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Active learning: select informative samples from unlabeled data\n",
        "    selected_samples = active_learning_strategy.select_samples(num_active_samples)\n",
        "    # Label selected samples (you need to implement this)\n",
        "    # X_labeled, y_labeled = ...\n",
        "    # Update labeled and unlabeled data sets\n",
        "    # X_unlabeled = ...\n",
        "    # Train GAN with labeled and unlabeled data\n",
        "    mogaal_model.train_on_batch(X_labeled, y_labeled)\n",
        "\n",
        "# Generate synthetic data samples\n",
        "noise_samples = np.random.normal(0, 1, (X_train_labeled.shape[0], 100))\n",
        "synthetic_data = generator.predict(noise_samples)\n",
        "\n",
        "# Evaluate performance (you need to implement this)\n",
        "# For example, you can use synthetic data for anomaly detection and evaluate accuracy\n",
        "# accuracy = ...\n",
        "\n",
        "# Print evaluation metrics\n",
        "# print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "-lW5t3kS7wqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}